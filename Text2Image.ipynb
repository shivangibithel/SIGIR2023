{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b13764",
   "metadata": {},
   "source": [
    "### Task: Text - to - Image using Image Captioning methods -- {BLIP, ClipCap, GIT, Vit-Gpt2} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb18b0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8dc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754b3f4",
   "metadata": {},
   "source": [
    "### Original Flickr8k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original captions - Queries\")\n",
    "print()\n",
    "with open(\"flickr8k_captionid.pkl\", \"rb\") as f:\n",
    "    flickr8k_captionid = pickle.load(f)\n",
    "print(\"Caption IDs: \", len(flickr8k_captionid), flickr8k_captionid[:2])\n",
    "print()\n",
    "with open(\"flickr8k_caption_text.pkl\", \"rb\") as f:\n",
    "    flickr8k_caption_text = pickle.load(f)\n",
    "print(\"Caption Text: \",len(flickr8k_caption_text), flickr8k_caption_text[:2])\n",
    "print()\n",
    "print(\"Original Images - Retrieval set\")\n",
    "print()\n",
    "with open(\"flickr8k_imagefile.pkl\", \"rb\") as f:\n",
    "    flickr8k_imagefile = pickle.load(f)\n",
    "print(\"Image IDs: \",len(flickr8k_imagefile), flickr8k_imagefile[:2])\n",
    "print()\n",
    "\n",
    "src = \"./original_images/\"\n",
    "name = src + flickr8k_imagefile[0]\n",
    "im = Image.open(name)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "name = src + flickr8k_imagefile[1]\n",
    "im = Image.open(name)\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6306656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICTIONARY\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"flickr8k_expert_annotations.csv\")\n",
    "# print(\"Unique Image ids\", len(set(df['iid'])))\n",
    "# print(\"Unique Caption ids\", len(set(df['cid'])))\n",
    "# df\n",
    "# dictionary={}\n",
    "# for i in range(len(df)):\n",
    "#     if str(df.iloc[i]['cid']) in flickr8k_captionid:\n",
    "#         if str(df.iloc[i]['cid']) not in list(dictionary.keys()):\n",
    "#             d = {}\n",
    "#             d[df.iloc[i]['iid']] = df.iloc[i]['annotation']\n",
    "#             dictionary[str(df.iloc[i]['cid'])] = d\n",
    "#         else:\n",
    "#             d_new = dictionary[str(df.iloc[i]['cid'])]\n",
    "#             d_new[df.iloc[i]['iid']] = df.iloc[i]['annotation']\n",
    "#             dictionary[str(df.iloc[i]['cid'])] = d_new\n",
    "    \n",
    "# print(len(dictionary))\n",
    "# dictionary['2549968784_39bfbe44f9.jpg#2']\n",
    "# a 1 indicating that the caption does not describe the image at all, \n",
    "# a 2 indicating the caption describes minor aspects of the image but does not describe the image, \n",
    "# a 3 indicating that the caption almost describes the image with minor mistakes, and \n",
    "# a 4 indicating that the caption describes the image.\n",
    "\n",
    "# path = \"./dictionary_text2image.pkl\"\n",
    "# with open(path, 'wb') as f:\n",
    "#     pickle.dump(dictionary, f)\n",
    "\n",
    "path = \"./dictionary_text2image.pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "dictionary[\"106490881_5a2dd9b7bd.jpg#2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e965d",
   "metadata": {},
   "source": [
    "### Loading the Generated Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c23ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(\"Generated retrieval set\")\n",
    "\n",
    "print(\"BLIP\")\n",
    "with open(\"generated_captions/BLIP/flickr_cap_gen_blip.pkl\", \"rb\") as f:\n",
    "    gen_blip = pickle.load(f)\n",
    "print(\"Caption Text: \",len(gen_blip), gen_blip[0],\"\\n\", gen_blip[1])\n",
    "print()  \n",
    "\n",
    "print(\"clipcap\")\n",
    "with open(\"generated_captions/clipcap/flickr_cap_gen_clipcap.pkl\", \"rb\") as f:\n",
    "    gen_clipcap = pickle.load(f)\n",
    "print(\"Caption Text: \",len(gen_clipcap), gen_clipcap[0],\"\\n\", gen_clipcap[1])\n",
    "print() \n",
    "\n",
    "print(\"GIT\")\n",
    "with open(\"generated_captions/git/flickr_cap_gen_git.pkl\", \"rb\") as f:\n",
    "    gen_git = pickle.load(f)\n",
    "print(\"Caption Text: \",len(gen_git), gen_git[0],\"\\n\", gen_git[1])\n",
    "print() \n",
    "\n",
    "print(\"vit_gpt2\")\n",
    "with open(\"generated_captions/vit_gpt2/flickr_cap_gen_vit_gpt2.pkl\", \"rb\") as f:\n",
    "    gen_vit_gpt2 = pickle.load(f)\n",
    "print(\"Caption Text: \",len(gen_vit_gpt2), gen_vit_gpt2[0],\"\\n\", gen_vit_gpt2[1])\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634646d",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c15ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from torch import nn\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as nnf\n",
    "# import sys\n",
    "# from typing import Tuple, List, Union, Optional\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "# from tqdm import tqdm, trange\n",
    "# import PIL.Image\n",
    "# from IPython.display import Image \n",
    "# import pickle\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-distilroberta-v1')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/all-distilroberta-v1')\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(flickr8k_caption_text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# # Normalize embeddings\n",
    "# sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# np.save(\"flickr8k_caption_original_queries_sbert.npy\",sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb12d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize sentences\n",
    "# # encoded_input = tokenizer(gen_blip, padding=True, truncation=True, return_tensors='pt') #blip\n",
    "# # encoded_input = tokenizer(gen_clipcap, padding=True, truncation=True, return_tensors='pt') #clipcap\n",
    "# # encoded_input = tokenizer(gen_git, padding=True, truncation=True, return_tensors='pt') #git\n",
    "# encoded_input = tokenizer(gen_vit_gpt2, padding=True, truncation=True, return_tensors='pt') #vit-gpt2\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Perform pooling\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# # Normalize embeddings\n",
    "# sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "# # np.save(\"gen_blip.npy\",sentence_embeddings)\n",
    "# # np.save(\"gen_clipcap.npy\",sentence_embeddings)\n",
    "# # np.save(\"gen_git.npy\",sentence_embeddings)\n",
    "# np.save(\"gen_vit_gpt2.npy\",sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864995fd",
   "metadata": {},
   "source": [
    "## Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\n",
    "#     \"Hello there good man!\",\n",
    "#     \"It is quite windy in London\",\n",
    "#     \"How is the weather today?\"\n",
    "# ]\n",
    "\n",
    "# tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "# bm25 = BM25Okapi(tokenized_corpus)\n",
    "# query = \"windy London\"\n",
    "# tokenized_query = query.split(\" \")\n",
    "\n",
    "# doc_scores = bm25.get_scores(tokenized_query)\n",
    "# print(doc_scores)\n",
    "# bm25.get_top_n(tokenized_query, corpus, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gen_blip\n",
    "tokenized_corpus = [doc.split(' ') for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "for i in range(len(flickr8k_caption_text)):\n",
    "    query = str(flickr8k_caption_text[i])\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    if i == 0:\n",
    "        cosine_sim = doc_scores\n",
    "    else:\n",
    "        cosine_sim =np.vstack([cosine_sim, doc_scores])\n",
    "\n",
    "print(cosine_sim.shape)\n",
    "sorted_cosine_sim= np.flip(np.sort(cosine_sim), axis = 1)   # sorted cosin similarity values\n",
    "sim_sorted_img_idx=np.flip(np.argsort(cosine_sim), axis = 1)   # positions of the sorted cosine similarity\n",
    "# bm25.get_top_n(tokenized_query, corpus, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498fe4e3",
   "metadata": {},
   "source": [
    "### Generated Rankings based on cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"text2image_features/flickr8k_caption_original_queries_sbert.npy\")  # queries\n",
    "\n",
    "Y = np.load(\"text2image_features/gen_blip.npy\")   # generated retrieval set\n",
    "# Y = np.load(\"text2image_features/gen_clipcap.npy\")   # generated retrieval set\n",
    "# Y = np.load(\"text2image_features/gen_git.npy\")   # generated retrieval set\n",
    "# Y = np.load(\"text2image_features/gen_vit_gpt2.npy\")   # generated retrieval set\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "cosine_sim = cosine_similarity(X, Y, dense_output=True)\n",
    "print(cosine_sim.shape)\n",
    "sorted_cosine_sim= np.flip(np.sort(cosine_sim), axis = 1)   # sorted cosin similarity values\n",
    "sim_sorted_img_idx=np.flip(np.argsort(cosine_sim), axis = 1)   # positions of the sorted cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b598254",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_caption_ids = np.zeros((977, 1000), dtype = np.dtype('U25'))  #mapped image ids \n",
    "for i in range(977):\n",
    "    for k in range(1000):\n",
    "        mapped_caption_ids[i][k] = flickr8k_imagefile[sim_sorted_img_idx[i][k]]\n",
    "\n",
    "# np.save(\"./blip/sorted_cosine_sim.npy\", sorted_cosine_sim)\n",
    "# np.save(\"./blip/mapped_caption_ids.npy\", mapped_caption_ids)\n",
    "\n",
    "# np.save(\"./clipcap/sorted_cosine_sim.npy\", sorted_cosine_sim)\n",
    "# np.save(\"./clipcap/mapped_caption_ids.npy\", mapped_caption_ids)\n",
    "\n",
    "# np.save(\"./git/sorted_cosine_sim.npy\", sorted_cosine_sim)\n",
    "# np.save(\"./git/mapped_caption_ids.npy\", mapped_caption_ids)\n",
    "\n",
    "# np.save(\"./vit_gpt2/sorted_cosine_sim.npy\", sorted_cosine_sim)\n",
    "# np.save(\"./vit_gpt2/mapped_caption_ids.npy\", mapped_caption_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197090c5",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated Dataset\n",
    "\n",
    "with open(\"generated_captions/BLIP/flickr8k_test_1.json\", \"rb\") as f:\n",
    "    flickr8k_caption_text1 = json.load(f)\n",
    "mapped_caption_ids1 = np.load(\"./blip/mapped_caption_ids.npy\")\n",
    "\n",
    "with open(\"generated_captions/clipcap/flickr8k_test_1.json\", \"rb\") as f:\n",
    "    flickr8k_caption_text2 = json.load(f)\n",
    "mapped_caption_ids2 = np.load(\"./clipcap/mapped_caption_ids.npy\")\n",
    "\n",
    "with open(\"generated_captions/git/flickr8k_test_1.json\", \"rb\") as f:\n",
    "    flickr8k_caption_text3 = json.load(f)\n",
    "mapped_caption_ids3 = np.load(\"./git/mapped_caption_ids.npy\")\n",
    "\n",
    "with open(\"generated_captions/vit_gpt2/flickr8k_test_1.json\", \"rb\") as f:\n",
    "    flickr8k_caption_text4 = json.load(f)\n",
    "mapped_caption_ids4 = np.load(\"./vit_gpt2/mapped_caption_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ea39f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rng = 2  # queries\n",
    "for j in range(1,rng):\n",
    "    print(\"Query:\", flickr8k_caption_text[j])\n",
    "    src = \"./original_images/\"\n",
    "    name = src + flickr8k_imagefile[j]\n",
    "    im = Image.open(name)\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"BLIP Retrieved set\")\n",
    "    for r in range(2):\n",
    "        print(flickr8k_caption_text1[mapped_caption_ids1[j][r]])\n",
    "        image = mapped_caption_ids1[j][r] \n",
    "        print()\n",
    "        name = \"original_images/\" + image\n",
    "        im = Image.open(name)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"CLIPCAP Retrieved set\")\n",
    "    for r in range(2):\n",
    "        print(flickr8k_caption_text2[mapped_caption_ids2[j][r]])\n",
    "        image = mapped_caption_ids2[j][r] \n",
    "        print()\n",
    "        name = \"original_images/\" + image\n",
    "        im = Image.open(name)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"GIT Retrieved set\")\n",
    "    for r in range(2):\n",
    "        print(flickr8k_caption_text3[mapped_caption_ids3[j][r]])\n",
    "        image = mapped_caption_ids3[j][r] \n",
    "        print()\n",
    "        name = \"original_images/\" + image\n",
    "        im = Image.open(name)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"VIT-GPT2 Retrieved set\")\n",
    "    for r in range(2):\n",
    "        print(flickr8k_caption_text4[mapped_caption_ids4[j][r]])\n",
    "        image = mapped_caption_ids4[j][r] \n",
    "        print()\n",
    "        name = \"original_images/\" + image\n",
    "        im = Image.open(name)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"##########################################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309634b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Demo BLIP\n",
    "\n",
    "rng = 2  # queries\n",
    "for j in range(1,rng):\n",
    "    print(\"Query:\", flickr8k_caption_text[j])\n",
    "    src = \"./original_images/\"\n",
    "    name = src + flickr8k_imagefile[j]\n",
    "    im = Image.open(name)\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"BLIP Generation Result\", gen_blip[j])\n",
    "    print(\"CLIPCAP Generation Result\", gen_clipcap[j])\n",
    "    print(\"VIT-GPT2 Generation Result\", gen_vit_gpt2[j])\n",
    "    print(\"GIT Generation Result\", gen_git[j])\n",
    "    print(\"-----------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# # CLIPCAP\n",
    "# predictions1 = [[\"A\", \"young\", \"boy\", \"is\", \"playing\", \"with\", \"a\", \"frisbee\", \"on\", \"the\", \"beach\" \".\"]]\n",
    "\n",
    "# # VIT-GPT2\n",
    "# predictions2 = [[\"a\", \"young\" ,\"boy\" ,\"standing\" ,\"on\" ,\"a\" ,\"beach\" ,\"holding\", \"a\" ,\"surfboard\"]]\n",
    "\n",
    "# # BLIP\n",
    "# predictions3 = [[\"a\", \"young\", \"boy\", \"standing\", \"on\", \"top\", \"of\", \"a\", \"beach\", \"next\", \"to\", \"the\", \"ocean\"]]\n",
    "\n",
    "# # GIT\n",
    "# predictions4 = [[\"a\", \"boy\", \"in\", \"blue\", \"shorts\", \"and\", \"blue\", \"shorts\", \"playing\", \"in\", \"the\", \"water\",\".\"]]\n",
    "\n",
    "# references = [\n",
    "#     [[\"A\", \"young\", \"boy\", \"in\", \"swimming\", \"trunks\", \"is\", \"walking\", \"with\", \"his\", \"arms\", \"outstretched\", \"on\", \"the\", \"beach\"],\n",
    "#     [\"A\", \"boy\", \"in\", \"his\", \"blue\", \"swim\", \"shorts\", \"at\", \"the\", \"beach\", \".\"],\n",
    "#     [\"A\", \"boy\", \"smiles\", \"for\", \"the\", \"camera\", \"at\", \"a\", \"beach\", \".\"],\n",
    "#     [\"Children\", \"playing\", \"on\", \"the\", \"beach\" , \".\"],\n",
    "#     [\"The\", \"boy\", \"is\", \"playing\", \"on\", \"the\", \"shore\", \"of\", \"an\", \"ocean\" , \".\"]]\n",
    "# ]\n",
    "\n",
    "# print(\"CLIPCAP\", bleu.compute(predictions=predictions1, references=references))\n",
    "# print()\n",
    "# print(\"VIT-GPT2\", bleu.compute(predictions=predictions2, references=references))\n",
    "# print()\n",
    "# print(\"BLIP\", bleu.compute(predictions=predictions3, references=references))\n",
    "# print()\n",
    "# print(\"GIT\", bleu.compute(predictions=predictions4, references=references))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10edad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# reference = [\n",
    "#     'A boy in his blue swim shorts at the beach .'.split(),\n",
    "#     'A boy smiles for the camera at a beach .'.split(),\n",
    "#     'A young boy in swimming trunks is walking with his arms outstretched on the beach .'.split(),\n",
    "#     'Children playing on the beach .'.split(),\n",
    "#     'The boy is playing on the shore of an ocean .'.split()\n",
    "# ]\n",
    "# # CLIPCAP\n",
    "# predictions1 = [\"A\", \"young\", \"boy\", \"is\", \"playing\", \"with\", \"a\", \"frisbee\", \"on\", \"the\", \"beach\" \".\"]\n",
    "\n",
    "# # VIT-GPT2\n",
    "# predictions2 = [\"a\", \"young\" ,\"boy\" ,\"standing\" ,\"on\" ,\"a\" ,\"beach\" ,\"holding\", \"a\" ,\"surfboard\"]\n",
    "\n",
    "# # BLIP\n",
    "# predictions3 = [\"a\", \"young\", \"boy\", \"standing\", \"on\", \"top\", \"of\", \"a\", \"beach\", \"next\", \"to\", \"the\", \"ocean\"]\n",
    "\n",
    "# # GIT\n",
    "# predictions4 = [\"a\", \"boy\", \"in\", \"blue\", \"shorts\", \"and\", \"blue\", \"shorts\", \"playing\", \"in\", \"the\", \"water\",\".\"]\n",
    "\n",
    "\n",
    "# # print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "# # print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "# # print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "# # print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\n",
    "# print(\"CLIPCAP-BLEU4\", sentence_bleu(reference, predictions1, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "# print(\"VIT-GPT2-BLEU4\", sentence_bleu(reference, predictions2, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "# print(\"BLIP-BLEU4\", sentence_bleu(reference, predictions3, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "# print(\"GIT-BLEU4\", sentence_bleu(reference, predictions4, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A boy in his blue swim shorts at the beach .\n",
    "# A boy smiles for the camera at a beach .\n",
    "# A young boy in swimming trunks is walking with his arms outstretched on the beach .\n",
    "# Children playing on the beach .\n",
    "# The boy is playing on the shore of an ocean ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# # CLIPCAP\n",
    "# predictions1 = [[\"A\", \"white\", \"dog\", \"laying\", \"on\", \"top\", \"of\", \"a\", \"wooden\", \"floor\", \".\"]]\n",
    "\n",
    "# # VIT-GPT2\n",
    "# predictions2 = [[\"a\", \"white\", \"dog\", \"laying\", \"on\", \"the\", \"floor\", \"next\", \"to\", \"a\", \"pillow\"]]\n",
    "\n",
    "# # BLIP\n",
    "# predictions3 = [[\"a\", \"large\", \"white\", \"dog\", \"laying\", \"on\", \"a\", \"tiled\", \"floor\"]]\n",
    "\n",
    "# # GIT\n",
    "# predictions4 = [[\"a\", \"white\", \"dog\", \"laying\", \"on\", \"the\", \"ground\", \"next\", \"to\", \"a\", \"pool\", \"of\", \"water\", \".\"]]\n",
    "\n",
    "# references = [\n",
    "#     [[\"A\", \"closeup\", \"of\", \"a\", \"white\", \"dog\", \"that\", \"is\", \"laying\", \"its\", \"head\", \"on\", \"its\", \"paws\", \".\"],\n",
    "#     [\"A\", \"white\", \"dog\", \"has\", \"its\", \"head\", \"on\", \"the\", \"ground\"],\n",
    "#     [\"a\", \"large\", \"white\", \"dog\", \"lying\", \"on\", \"the\", \"floor\" , \".\"],\n",
    "#     [\"A\", \"white\", \"dog\", \"rests\", \"its\", \"head\", \"on\", \"the\", \"patio\", \"bricks\" , \".\"],\n",
    "#     [\"A\", \"white\", \"dog\", \"is\", \"resting\", \"its\", \"head\", \"on\", \"a\", \"tiled\", \"floor\", \"with\", \"its\", \"eyes\", \"open\", \".\"]]\n",
    "# ]\n",
    "\n",
    "\n",
    "# print(\"CLIPCAP\", bleu.compute(predictions=predictions1, references=references))\n",
    "# print()\n",
    "# print(\"VIT-GPT2\", bleu.compute(predictions=predictions2, references=references))\n",
    "# print()\n",
    "# print(\"BLIP\", bleu.compute(predictions=predictions3, references=references))\n",
    "# print()\n",
    "# print(\"GIT\", bleu.compute(predictions=predictions4, references=references))\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f970bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [\n",
    "    \"A closeup of a white dog that is laying its head on its paws .\".split(),\n",
    "    \"a large white dog lying on the floor .\".split(),\n",
    "    \"A white dog has its head on the ground .\".split(),\n",
    "    \"A white dog is resting its head on a tiled floor with its eyes open .\".split(),\n",
    "    \"A white dog rests its head on the patio bricks .\".split()\n",
    "]\n",
    "# CLIPCAP\n",
    "predictions1 = [\"A\", \"white\", \"dog\", \"laying\", \"on\", \"top\", \"of\", \"a\", \"wooden\", \"floor\", \".\"]\n",
    "\n",
    "# VIT-GPT2\n",
    "predictions2 = [\"a\", \"white\", \"dog\", \"laying\", \"on\", \"the\", \"floor\", \"next\", \"to\", \"a\", \"pillow\"]\n",
    "\n",
    "# BLIP\n",
    "predictions3 = [\"a\", \"large\", \"white\", \"dog\", \"laying\", \"on\", \"a\", \"tiled\", \"floor\"]\n",
    "\n",
    "# GIT\n",
    "predictions4 = [\"a\", \"white\", \"dog\", \"laying\", \"on\", \"the\", \"ground\", \"next\", \"to\", \"a\", \"pool\", \"of\", \"water\", \".\"]\n",
    "\n",
    "\n",
    "print(\"CLIPCAP-BLEU4\", sentence_bleu(reference, predictions1, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"VIT-GPT2-BLEU4\", sentence_bleu(reference, predictions2, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"BLIP-BLEU4\", sentence_bleu(reference, predictions3, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "print(\"GIT-BLEU4\", sentence_bleu(reference, predictions4, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A closeup of a white dog that is laying its head on its paws .\n",
    "a large white dog lying on the floor .\n",
    "A white dog has its head on the ground .\n",
    "A white dog is resting its head on a tiled floor with its eyes open .\n",
    "A white dog rests its head on the patio bricks ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c6c5b",
   "metadata": {},
   "source": [
    "### Trec Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREC RUNS\n",
    "\n",
    "# sim_sorted_img_ids = np.load(\"./blip/mapped_caption_ids.npy\")  # doc no\n",
    "# sim_sorted_img_ids = np.load(\"./clipcap/mapped_caption_ids.npy\")  # doc no\n",
    "# sim_sorted_img_ids = np.load(\"./vit_gpt2/mapped_caption_ids.npy\")  # doc no\n",
    "sim_sorted_img_ids = np.load(\"./git/mapped_caption_ids.npy\")  # doc no\n",
    "print(sim_sorted_img_ids.shape)\n",
    "\n",
    "# sorted_cosine_sim = np.load(\"./blip/sorted_cosine_sim.npy\")  # score\n",
    "# sorted_cosine_sim = np.load(\"./clipcap/sorted_cosine_sim.npy\")  # score\n",
    "# sorted_cosine_sim = np.load(\"./vit_gpt2/sorted_cosine_sim.npy\")  # score\n",
    "# sorted_cosine_sim = np.load(\"./git/sorted_cosine_sim.npy\")  # score\n",
    "# print(sorted_cosine_sim.shape)\n",
    "\n",
    "# trec_run=[]\n",
    "# for i in range(977):\n",
    "#     for j in range(1000):\n",
    "#         qid = i + 1\n",
    "#         Q0 = \"Q0\"\n",
    "#         docno = sim_sorted_img_ids[i][j]\n",
    "#         rank = j + 1\n",
    "#         score = sorted_cosine_sim[i][j]\n",
    "# #         tag = \"blip_4_full\"\n",
    "# #         tag = \"clipcap_4_full\"\n",
    "# #         tag = \"vit_gpt2_4_full\"\n",
    "#         tag = \"git_4_full\"\n",
    "#         line = str(qid) + \" \" + Q0 + \" \" + str(docno) + \" \" + str(rank) + \" \" + str(score) + \" \" + tag\n",
    "#         trec_run.append(line)\n",
    "# trec_run\n",
    "\n",
    "# # with open('trec/blip_4_full.txt', 'w') as f:\n",
    "# # with open('trec/clipcap_4_full.txt', 'w') as f:\n",
    "# # with open('trec/vit_gpt2_4_full.txt', 'w') as f:\n",
    "# with open('trec/git_4_full.txt', 'w') as f:\n",
    "#     for line in trec_run:\n",
    "#         f.write(line)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee56767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RANKINGS\n",
    "  \n",
    "# # sim_sorted_img_ids  # doc no    \n",
    "# # query list = flickr8k_captionid\n",
    "\n",
    "# trec_rel=[]\n",
    "# for i in range(100):\n",
    "#     d = dictionary[flickr8k_captionid[i]]\n",
    "#     all_image_ids = sim_sorted_img_ids[i]\n",
    "#     d_keys = list(d.keys())\n",
    "#     for j in range(50):\n",
    "#         qid = i + 1    \n",
    "#         docno = all_image_ids[j]\n",
    "#         if docno in d_keys:\n",
    "#             relevance = d[docno]\n",
    "#         else:\n",
    "#             relevance = 0\n",
    "            \n",
    "#         line = str(qid) + \" \" + str(docno) + \" \" + str(relevance)\n",
    "#         trec_rel.append(line)\n",
    "\n",
    "# with open('trec/rankings_blip.txt', 'w') as f:\n",
    "# with open('trec/rankings_clipcap.txt', 'w') as f:\n",
    "# with open('trec/rankings_vit_gpt2.txt', 'w') as f:\n",
    "# with open('trec/rankings_git.txt', 'w') as f:\n",
    "#     for line in trec_rel:\n",
    "#         f.write(line)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a073c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrecQrel format\n",
    "\n",
    "# sim_sorted_img_ids  # doc no    \n",
    "# query list = flickr8k_captionid\n",
    "\n",
    "# qid 0 docno relevance\n",
    "\n",
    "# where:\n",
    "\n",
    "# qid is the query number\n",
    "# 0 is the literal 0\n",
    "# docno is the id of a document in your collection\n",
    "# relevance is how relevant is docno for qid\n",
    "# Example:\n",
    "# 1 0 aldf.1864_12_000027 1\n",
    "# 1 0 aller1867_12_000032 2\n",
    "# 1 0 aller1868_12_000012 0\n",
    "# 1 0 aller1871_12_000640 1\n",
    "# 1 0 arthr0949_12_000945 0\n",
    "# 1 0 arthr0949_12_000974 1\n",
    "\n",
    "with open(\"flickr8k_captionid.pkl\", \"rb\") as f:\n",
    "    flickr8k_captionid = pickle.load(f)\n",
    "\n",
    "path = \"./dictionary_text2image.pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    \n",
    "\n",
    "\n",
    "trec_rel=[]\n",
    "for i in range(977):\n",
    "    d = dictionary[flickr8k_captionid[i]]\n",
    "    all_caption_ids = sim_sorted_img_ids[i]\n",
    "    d_keys = list(d.keys())\n",
    "    for j in range(1000):\n",
    "        qid = i + 1\n",
    "        Q0 = \"0\"\n",
    "        docno = all_caption_ids[j]\n",
    "        if docno in d_keys:\n",
    "            relevance = d[docno]-1\n",
    "        else:\n",
    "            relevance = 3\n",
    "        line = str(qid) + \" \" + Q0 + \" \" + str(docno) + \" \" + str(relevance)\n",
    "        trec_rel.append(line)\n",
    "\n",
    "with open('trec/qrels_3_highest_text_full_3_others.txt', 'w') as f:\n",
    "    for line in trec_rel:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"flickr8k_captionid.pkl\", \"rb\") as f:\n",
    "    flickr8k_captionid = pickle.load(f)\n",
    "\n",
    "path = \"./dictionary_text2image.pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    \n",
    "# sim_sorted_img_ids -- doc no    \n",
    "# query list =-- flickr8k_captionid\n",
    "\n",
    "trec_rel=[]\n",
    "for i in range(977):\n",
    "    d = dictionary[flickr8k_captionid[i]]\n",
    "    d_keys = list(d.keys())\n",
    "    for j in range(len(d)):\n",
    "        qid = i + 1\n",
    "        Q0 = \"0\"\n",
    "        docno = d_keys[j]\n",
    "        score = d[docno]\n",
    "        relevance = score - 1\n",
    "        line = str(qid) + \" \" + Q0 + \" \" + str(docno) + \" \" + str(relevance)\n",
    "        trec_rel.append(line)\n",
    "\n",
    "with open('trec/qrels_3_text_condensed.txt', 'w') as f:\n",
    "    for line in trec_rel:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce593f",
   "metadata": {},
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404d102",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"trec/qrels_3_highest_text_full_3_others\")\n",
    "qrels = TrecQrel(\"trec/qrels_3_highest_text_full_3_others.txt\")\n",
    "print(qrels.describe())\n",
    "\n",
    "r1 = TrecRun(\"trec/blip_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"BLIP Results\")\n",
    "print(\"NDCG at 5\", te1.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te1.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te1.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te1.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te1.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te1.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te1.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te1.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r2 = TrecRun(\"trec/clipcap_4_full.txt\")\n",
    "te2 = TrecEval(r2, qrels)\n",
    "print(\"CLIPCAP Results\")\n",
    "print(\"NDCG at 5\", te2.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te2.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te2.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te2.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te2.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te2.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te2.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te2.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "r3 = TrecRun(\"trec/vit_gpt2_4_full.txt\")\n",
    "te3 = TrecEval(r3, qrels)\n",
    "print(\"VIT-GPT2 Results\")\n",
    "print(\"NDCG at 5\", te3.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te3.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te3.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te3.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te3.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te3.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te3.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te3.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r4 = TrecRun(\"trec/git_4_full.txt\")\n",
    "te4 = TrecEval(r4, qrels)\n",
    "print(\"GIT Results\")\n",
    "print(\"NDCG at 5\", te4.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te4.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te4.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te4.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te4.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te4.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te4.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te4.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941e95f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"trec/qrels_3_highest_text_full_0_others\")\n",
    "qrels = TrecQrel(\"trec/qrels_3_highest_text_full_0_others.txt\")\n",
    "print(qrels.describe())\n",
    "\n",
    "r1 = TrecRun(\"trec/blip_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"BLIP Results\")\n",
    "print(\"NDCG at 5\", te1.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te1.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te1.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te1.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te1.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te1.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te1.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te1.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r2 = TrecRun(\"trec/clipcap_4_full.txt\")\n",
    "te2 = TrecEval(r2, qrels)\n",
    "print(\"CLIPCAP Results\")\n",
    "print(\"NDCG at 5\", te2.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te2.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te2.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te2.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te2.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te2.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te2.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te2.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "r3 = TrecRun(\"trec/vit_gpt2_4_full.txt\")\n",
    "te3 = TrecEval(r3, qrels)\n",
    "print(\"VIT-GPT2 Results\")\n",
    "print(\"NDCG at 5\", te3.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te3.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te3.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te3.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te3.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te3.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te3.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te3.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r4 = TrecRun(\"trec/git_4_full.txt\")\n",
    "te4 = TrecEval(r4, qrels)\n",
    "print(\"GIT Results\")\n",
    "print(\"NDCG at 5\", te4.get_ndcg(depth=5))\n",
    "print(\"NDCG at 10\", te4.get_ndcg(depth=10))\n",
    "print(\"NDCG at 15\", te4.get_ndcg(depth=15))\n",
    "print(\"NDCG at 20\", te4.get_ndcg(depth=20))\n",
    "print()\n",
    "print(\"removeUnjudged = False, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te4.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te4.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te4.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te4.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4de4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "\n",
    "print(\"trec/qrels_3_text_condensed\")\n",
    "qrels = TrecQrel(\"trec/qrels_3_text_condensed.txt\")\n",
    "print(qrels.describe())\n",
    "\n",
    "r1 = TrecRun(\"trec/blip_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"BLIP Results\")\n",
    "print(\"NDCG at 5\", te1.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te1.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te1.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te1.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te1.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te1.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te1.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te1.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = True))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r2 = TrecRun(\"trec/clipcap_4_full.txt\")\n",
    "te2 = TrecEval(r2, qrels)\n",
    "print(\"CLIPCAP Results\")\n",
    "print(\"NDCG at 5\", te2.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te2.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te2.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te2.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te2.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te2.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te2.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te2.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = True))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "r3 = TrecRun(\"trec/vit_gpt2_4_full.txt\")\n",
    "te3 = TrecEval(r3, qrels)\n",
    "print(\"VIT-GPT2 Results\")\n",
    "print(\"NDCG at 5\", te3.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te3.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te3.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te3.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te3.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te3.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te3.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te3.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = True))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r4 = TrecRun(\"trec/git_4_full.txt\")\n",
    "te4 = TrecEval(r4, qrels)\n",
    "print(\"GIT Results\")\n",
    "print(\"NDCG at 5\", te4.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te4.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te4.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te4.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te4.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te4.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te4.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = True))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te4.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601853df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "\n",
    "print(\"trec/qrels_3_text_condensed\")\n",
    "qrels = TrecQrel(\"trec/qrels_3_text_condensed.txt\")\n",
    "print(qrels.describe())\n",
    "\n",
    "r1 = TrecRun(\"trec/blip_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"BLIP Results\")\n",
    "print(\"NDCG at 5\", te1.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te1.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te1.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te1.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te1.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te1.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te1.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te1.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r2 = TrecRun(\"trec/clipcap_4_full.txt\")\n",
    "te2 = TrecEval(r2, qrels)\n",
    "print(\"CLIPCAP Results\")\n",
    "print(\"NDCG at 5\", te2.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te2.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te2.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te2.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te2.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te2.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te2.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te2.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "r3 = TrecRun(\"trec/vit_gpt2_4_full.txt\")\n",
    "te3 = TrecEval(r3, qrels)\n",
    "print(\"VIT-GPT2 Results\")\n",
    "print(\"NDCG at 5\", te3.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te3.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te3.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te3.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te3.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te3.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te3.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te3.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "r4 = TrecRun(\"trec/git_4_full.txt\")\n",
    "te4 = TrecEval(r4, qrels)\n",
    "print(\"GIT Results\")\n",
    "print(\"NDCG at 5\", te4.get_ndcg(depth=5, removeUnjudged = True))\n",
    "print(\"NDCG at 10\", te4.get_ndcg(depth=10, removeUnjudged = True))\n",
    "print(\"NDCG at 15\", te4.get_ndcg(depth=15, removeUnjudged = True))\n",
    "print(\"NDCG at 20\", te4.get_ndcg(depth=20, removeUnjudged = True))\n",
    "print()\n",
    "print(\"removeUnjudged = True, binary_topical_relevance=False\")\n",
    "print(\"rbp@5: %.4f, residuals@5: %.2f, \" % te4.get_rbp(p=0.5, depth= 5, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@10: %.4f, residuals@10: %.2f, \" % te4.get_rbp(p=0.5, depth= 10, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@15: %.4f, residuals@15: %.2f, \" % te4.get_rbp(p=0.5, depth= 15, binary_topical_relevance=False, removeUnjudged = False))\n",
    "print(\"rbp@20: %.4f, residuals@20: %.2f, \" % te4.get_rbp(p=0.5, depth= 20, binary_topical_relevance=False, removeUnjudged = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "\n",
    "# query 2\n",
    "\n",
    "print(\"trec/qrels_3_text_condensed\")\n",
    "qrels = TrecQrel(\"trec/qrels_q2.txt\")\n",
    "# print(qrels.describe())\n",
    "\n",
    "r1 = TrecRun(\"trec/blip_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"BLIP Results\")\n",
    "print(te1.get_ndcg(depth=5, removeUnjudged = True, per_query = True))\n",
    "\n",
    "r1 = TrecRun(\"trec/clipcap_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"CLIPCAP Results\")\n",
    "print(te1.get_ndcg(depth=5, removeUnjudged = True, per_query = True))\n",
    "\n",
    "r1 = TrecRun(\"trec/vit_gpt2_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"VIT-GPT2 Results\")\n",
    "print(te1.get_ndcg(depth=5, removeUnjudged = True, per_query = True))\n",
    "\n",
    "r1 = TrecRun(\"trec/git_4_full.txt\")\n",
    "te1 = TrecEval(r1, qrels)\n",
    "print(\"GIT Results\")\n",
    "print(te1.get_ndcg(depth=5, removeUnjudged = True, per_query = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Score\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "_ = torch.manual_seed(42)\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Read a PIL image\n",
    "name = src + flickr8k_imagefile[1]\n",
    "image = Image.open(name)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "# Define a transform to convert PIL\n",
    "# image to a Torch tensor\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.PILToTensor()\n",
    "])\n",
    "\n",
    "# transform = transforms.PILToTensor()\n",
    "# Convert the PIL image to Torch tensor\n",
    "img_tensor = transform(image)\n",
    "\n",
    "print(metric(img_tensor, \"a young boy standing on top of a beach next to the ocean\"))\n",
    "print(metric(img_tensor, \"A young boy is playing with a frisbee on the beach.\"))\n",
    "print(metric(img_tensor, \"a young boy standing on a beach holding a surfboard\"))\n",
    "print(metric(img_tensor, \"a boy in blue shorts and blue shorts playing in the water.\"))\n",
    "\n",
    "\n",
    "# BLIP Generation Result a young boy standing on top of a beach next to the ocean\n",
    "# CLIPCAP Generation Result A young boy is playing with a frisbee on the beach.\n",
    "# VIT-GPT2 Generation Result a young boy standing on a beach holding a surfboard\n",
    "# GIT Generation Result a boy in blue shorts and blue shorts playing in the water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric(img_tensor, \"two boys playing near the ocean\"))\n",
    "print(metric(img_tensor, \"a picture having two boys, one standing and another bending to touch the ocean\"))\n",
    "print(metric(img_tensor, \"two boys playing near the ocean, one standing in blue shorts and another bending to touch the water\"))\n",
    "print(metric(img_tensor, \"a photograph of two boys, one standing on the sea in blue shorts and another bending to touch the water\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589579e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric(img_tensor, \"two young boys in blue shorts playing on the ocean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac916ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BLIP\n",
    "!python ../clipscore/clipscore.py ../clipscore/flickr8k/BLIP/flickr8k_test_clipscore.json  original_images/ --references_json ../clipscore/flickr8k/flickr8k_ref.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5e43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CLIPCAP\n",
    "!python ../clipscore/clipscore.py ../clipscore/flickr8k/clipcap/flickr8k_test_clipscore.json  original_images/ --references_json ../clipscore/flickr8k/flickr8k_ref.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8569cb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GIT\n",
    "!python ../clipscore/clipscore.py ../clipscore/flickr8k/git/flickr8k_test_clipscore.json  original_images/ --references_json ../clipscore/flickr8k/flickr8k_ref.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d20cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# VIT-GPT2\n",
    "!python ../clipscore/clipscore.py ../clipscore/flickr8k/vit_gpt2/flickr8k_test_clipscore.json  original_images/ --references_json ../clipscore/flickr8k/flickr8k_ref.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # VIT-GPT2\n",
    "\n",
    "# BLEU-1: 0.6158\n",
    "# BLEU-2: 0.4201\n",
    "# BLEU-3: 0.2719\n",
    "# BLEU-4: 0.1721\n",
    "# METEOR: 0.2010\n",
    "# ROUGE: 0.4538\n",
    "# CIDER: 0.4760\n",
    "# SPICE: 0.1390\n",
    "# CLIPScore: 0.7147\n",
    "# RefCLIPScore: 0.7522\n",
    "\n",
    "    \n",
    "# # GIT\n",
    "# BLEU-1: 0.6972\n",
    "# BLEU-2: 0.5166\n",
    "# BLEU-3: 0.3654\n",
    "# BLEU-4: 0.2568\n",
    "# METEOR: 0.2372\n",
    "# ROUGE: 0.5097\n",
    "# CIDER: 0.7464\n",
    "# SPICE: 0.1767\n",
    "# CLIPScore: 0.7853\n",
    "# RefCLIPScore: 0.8141\n",
    "    \n",
    "# # CLIPCAP    \n",
    "# BLEU-1: 0.6389\n",
    "# BLEU-2: 0.4496\n",
    "# BLEU-3: 0.2951\n",
    "# BLEU-4: 0.1869\n",
    "# METEOR: 0.2161\n",
    "# ROUGE: 0.4752\n",
    "# CIDER: 0.5466\n",
    "# SPICE: 0.1498\n",
    "# CLIPScore: 0.7755\n",
    "# RefCLIPScore: 0.7911\n",
    "    \n",
    "# # BLIP\n",
    "\n",
    "# BLEU-1: 0.7221\n",
    "# BLEU-2: 0.5501\n",
    "# BLEU-3: 0.4036\n",
    "# BLEU-4: 0.2914\n",
    "# METEOR: 0.2637\n",
    "# ROUGE: 0.5411\n",
    "# CIDER: 0.8993\n",
    "# SPICE: 0.1974\n",
    "# CLIPScore: 0.7781\n",
    "# RefCLIPScore: 0.8152"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a814c91",
   "metadata": {},
   "source": [
    "# Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59259fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"generated_captions/BLIP/flickr8k_test_1.json\", \"rb\") as f:\n",
    "#     flickr8k_caption_text1 = json.load(f)\n",
    "# with open(\"generated_captions/clipcap/flickr8k_test_1.json\", \"rb\") as f:\n",
    "#     flickr8k_caption_text2 = json.load(f)\n",
    "# with open(\"generated_captions/git/flickr8k_test_1.json\", \"rb\") as f:\n",
    "#     flickr8k_caption_text3 = json.load(f)\n",
    "# with open(\"generated_captions/vit_gpt2/flickr8k_test_1.json\", \"rb\") as f:\n",
    "#     flickr8k_caption_text4 = json.load(f)\n",
    "\n",
    "# # Generated Dataset formatting\n",
    "# dict1 = {}\n",
    "# d = flickr8k_caption_text4\n",
    "# for i in range(len(d)):\n",
    "#     d_keys = list(d.keys())\n",
    "#     dict1[d_keys[i][:-4]] = d[d_keys[i]]\n",
    "# dict1\n",
    "\n",
    "# # with open(\"generated_captions/BLIP/flickr8k_test_clipscore.json\", 'w') as f:\n",
    "# # with open(\"generated_captions/clipcap/flickr8k_test_clipscore.json\", 'w') as f:\n",
    "# # with open(\"generated_captions/git/flickr8k_test_clipscore.json\", 'w') as f:\n",
    "# with open(\"generated_captions/vit_gpt2/flickr8k_test_clipscore.json\", 'w') as f:\n",
    "#     json.dump(dict1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('annotations/captions.txt', sep=',')\n",
    "# print(df.shape[0])\n",
    "# df.head()\n",
    "\n",
    "# # CLIP Score FORMAT\n",
    "\n",
    "# dictionary={}\n",
    "# for i in range(len(df)):\n",
    "#     if str(df.iloc[i]['image']) in flickr8k_imagefile:\n",
    "#         if str(df.iloc[i]['image'][:-4]) not in list(dictionary.keys()):\n",
    "#             dictionary[str(df.iloc[i]['image'][:-4])] = [str(df.iloc[i]['caption'])]\n",
    "#         else:\n",
    "#             dictionary[str(df.iloc[i]['image'][:-4])].append(str(df.iloc[i]['caption']))\n",
    "# dictionary\n",
    "\n",
    "# with open(\"annotations/flickr8k_ref.json\", \"w\") as f:\n",
    "#     json.dump(dictionary,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea702cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('annotations/captions.txt', sep=',')\n",
    "# print(df.shape[0])\n",
    "# df.head()\n",
    "\n",
    "# # COCO FORMAT\n",
    "\n",
    "# # Create annotations\n",
    "# annotations = [{\"image_id\" : df['image'].iloc[i], \"id\" : i, \"caption\" : df['caption'].iloc[i]} for i in range(df.shape[0])]\n",
    "    \n",
    "# # Create image annotations\n",
    "# images = [{\"id\" : df['image'].iloc[i], \"file_name\" : df['image'].iloc[i]} for i in range(df.shape[0])]\n",
    "\n",
    "# # Create coco_format\n",
    "# coco_format = {\"annotations\" : annotations,\n",
    "#                \"images\" : images,\n",
    "#                \"type\": \"captions\",\n",
    "#                \"info\": \"dummy\",\n",
    "#                \"licenses\": \"dummy\"}\n",
    "   \n",
    "# with open(annotations/flickr8k_coco_format.json\", \"w\") as f:\n",
    "#     json.dump(coco_format, f, separators=(', ', ': '), ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bb40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycocotools.coco import COCO\n",
    "# from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "# # annotation_file = \"annotations/final_flickr_separateGT_test.json\"  -- Results do not correspond to current coco set\n",
    "# # annotation_file = \"annotations/dataset_flickr8k.json\"  -- not useful\n",
    "# annotation_file = \"annotations/flickr8k_caption_coco_format.json\"\n",
    "# results_file = \"generated_captions/BLIP/flickr8k_test_cs.json\"\n",
    "# # results_file =\"generated_captions/clipcap/flickr8k_test_cs.json\"\n",
    "# # results_file =\"generated_captions/git/flickr8k_test_cs.json\"\n",
    "# # results_file = \"generated_captions/vit_gpt2/flickr8k_test_cs.json\"\n",
    "\n",
    "# coco = COCO(annotation_file)\n",
    "# coco_result = coco.loadRes(results_file)\n",
    "# coco_eval = COCOEvalCap(coco, coco_result)\n",
    "# coco_eval.params['image_id'] = coco_result.getImgIds()\n",
    "# coco_eval.evaluate()\n",
    "\n",
    "# for metric, score in coco_eval.eval.items():\n",
    "#     print(f'{metric}: {score:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
